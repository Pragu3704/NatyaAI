{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pragu3704/NatyaAI/blob/main/RAPID_Internship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mounting Drive"
      ],
      "metadata": {
        "id": "miB8i8Tx5vJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhOK1zpGjggK"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ],
      "metadata": {
        "id": "oT85XCgwI_2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Joint angles : Video features\n"
      ],
      "metadata": {
        "id": "eUY8_N9v51rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "def calculate_angle(a, b, c):\n",
        "    a = np.array([a.x, a.y])\n",
        "    b = np.array([b.x, b.y])\n",
        "    c = np.array([c.x, c.y])\n",
        "\n",
        "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
        "    angle = np.abs(radians * 180.0 / np.pi)\n",
        "\n",
        "    if angle > 180.0:\n",
        "        angle = 360.0 - angle\n",
        "\n",
        "    return angle\n",
        "\n",
        "\n",
        "def extract_joint_angles(video_path):\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    angles = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose.process(frame_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            angle_data = []\n",
        "\n",
        "            # Left arm angles\n",
        "            left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n",
        "            left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
        "            left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
        "            left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
        "\n",
        "            left_shoulder_angle = calculate_angle(left_hip, left_shoulder, left_elbow)\n",
        "            left_elbow_angle = calculate_angle(left_elbow, left_shoulder, left_wrist)\n",
        "            left_wrist_angle = calculate_angle(left_elbow, left_wrist, landmarks[mp_pose.PoseLandmark.LEFT_INDEX.value])\n",
        "            angle_data.extend([left_shoulder_angle, left_elbow_angle, left_wrist_angle])\n",
        "\n",
        "            # Right arm angles\n",
        "            right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n",
        "            right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
        "            right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
        "            right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
        "\n",
        "            right_shoulder_angle = calculate_angle(right_hip, right_shoulder, right_elbow)\n",
        "            right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
        "            right_wrist_angle = calculate_angle(right_elbow, right_wrist, landmarks[mp_pose.PoseLandmark.RIGHT_INDEX.value])\n",
        "            angle_data.extend([right_shoulder_angle, right_elbow_angle, right_wrist_angle])\n",
        "\n",
        "            # Left leg angles\n",
        "            left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
        "            left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
        "\n",
        "            left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
        "            left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
        "            left_ankle_angle = calculate_angle(left_knee, left_ankle, landmarks[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value])\n",
        "            angle_data.extend([left_hip_angle, left_knee_angle, left_ankle_angle])\n",
        "\n",
        "            # Right leg angles\n",
        "            right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value]\n",
        "            right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
        "\n",
        "            right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
        "            right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
        "            right_ankle_angle = calculate_angle(right_knee, right_ankle, landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value])\n",
        "            angle_data.extend([right_hip_angle, right_knee_angle, right_ankle_angle])\n",
        "\n",
        "            # Append more joint angles as needed\n",
        "            angles.append(angle_data)\n",
        "\n",
        "    cap.release()\n",
        "    pose.close()\n",
        "    return np.array(angles)\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/Dataset'\n",
        "output_folder = '/content/drive/MyDrive/Output_Angles'\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "for video_file in os.listdir(input_folder):\n",
        "    video_path = os.path.join(input_folder, video_file)\n",
        "    angles = extract_joint_angles(video_path)\n",
        "\n",
        "    output_path = os.path.join(output_folder, f\"{os.path.splitext(video_file)[0]}_angles.npy\")\n",
        "    np.save(output_path, angles)\n",
        "    print(f\"Processed and saved: {video_file}\")\n",
        "\n",
        "print(\"All videos processed.\")\n"
      ],
      "metadata": {
        "id": "L4Aq5Kht_SVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction : Audio\n",
        "\n"
      ],
      "metadata": {
        "id": "Iwd_VYG66NXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def extract_audio_features(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "    # Extracting various audio features\n",
        "    features = {}\n",
        "\n",
        "    # Tempo and Beat\n",
        "    tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
        "    features['tempo'] = tempo\n",
        "    features['beat_frames'] = beat_frames\n",
        "\n",
        "    # MFCCs\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    features['mfcc'] = mfcc\n",
        "\n",
        "    # Chromagram\n",
        "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "    features['chroma'] = chroma\n",
        "\n",
        "    # Spectral Contrast\n",
        "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    features['spectral_contrast'] = spectral_contrast\n",
        "\n",
        "    # Tonnetz (tonal centroid features)\n",
        "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
        "    features['tonnetz'] = tonnetz\n",
        "\n",
        "    # Mel Spectrogram\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    features['mel_spectrogram'] = mel_spectrogram\n",
        "\n",
        "    # RMS (root mean square energy)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "    features['rms'] = rms\n",
        "\n",
        "    # Zero-Crossing Rate\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    features['zcr'] = zcr\n",
        "\n",
        "    # Spectral Centroid\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    features['spectral_centroid'] = spectral_centroid\n",
        "\n",
        "    # Spectral Bandwidth\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    features['spectral_bandwidth'] = spectral_bandwidth\n",
        "\n",
        "    # Spectral Rolloff\n",
        "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    features['spectral_rolloff'] = spectral_rolloff\n",
        "\n",
        "    return features\n",
        "\n",
        "def extract_audio_from_video(video_path, output_audio_path):\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(output_audio_path, codec='pcm_s16le')\n",
        "\n",
        "def process_videos(input_folder, output_audio_folder, output_features_folder):\n",
        "    if not os.path.exists(output_audio_folder):\n",
        "        os.makedirs(output_audio_folder)\n",
        "    if not os.path.exists(output_features_folder):\n",
        "        os.makedirs(output_features_folder)\n",
        "\n",
        "    for video_file in os.listdir(input_folder):\n",
        "        if video_file.endswith('.mp4'):\n",
        "            video_path = os.path.join(input_folder, video_file)\n",
        "            audio_path = os.path.join(output_audio_folder, os.path.splitext(video_file)[0] + '.wav')\n",
        "            feature_path = os.path.join(output_features_folder, os.path.splitext(video_file)[0] + '_features.npz')\n",
        "\n",
        "            # Extract audio from video\n",
        "            extract_audio_from_video(video_path, audio_path)\n",
        "\n",
        "            # Extract audio features\n",
        "            audio_features = extract_audio_features(audio_path)\n",
        "\n",
        "            # Save the features\n",
        "            np.savez(feature_path, **audio_features)\n",
        "            print(f\"Processed and saved features for: {video_file}\")\n",
        "\n",
        "# Paths to your input video folder and output folders for audio and features\n",
        "input_folder = '/content/drive/MyDrive/Dataset'\n",
        "output_audio_folder = '/content/drive/MyDrive/Output_audio'\n",
        "output_features_folder = '/content/drive/MyDrive/Output_features'\n",
        "\n",
        "# Process all videos in the input folder\n",
        "process_videos(input_folder, output_audio_folder, output_features_folder)\n",
        "print(\"All videos processed, audio extracted, and features saved.\")"
      ],
      "metadata": {
        "id": "_4OVpNcFBPHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpolation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kj9WaLGn6y0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video"
      ],
      "metadata": {
        "id": "ey6notGgJt9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Interpolate data\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "import os\n",
        "\n",
        "def resample_joint_angles(joint_angles, target_frames=900):\n",
        "    num_original_frames = len(joint_angles)\n",
        "\n",
        "    if num_original_frames == target_frames:\n",
        "        return joint_angles\n",
        "    elif num_original_frames < target_frames:\n",
        "        # Interpolate to increase frames\n",
        "        original_indices = np.linspace(0, 1, num=num_original_frames)\n",
        "        target_indices = np.linspace(0, 1, num=target_frames)\n",
        "        interpolated_angles = interp1d(original_indices, joint_angles, axis=0, kind='cubic')(target_indices)\n",
        "        return interpolated_angles\n",
        "    else:\n",
        "        # Downsample to reduce frames\n",
        "        ratio = num_original_frames // target_frames\n",
        "        downsampled_angles = np.mean(joint_angles[:target_frames * ratio].reshape(-1, ratio, joint_angles.shape[1]), axis=1)\n",
        "        return downsampled_angles\n",
        "\n",
        "# Folder paths\n",
        "processed_folder = '/content/drive/MyDrive/Output_Angles'\n",
        "resampled_folder = '/content/drive/MyDrive/Interpolated_Angles'\n",
        "target_frames = 900  # Target number of frames\n",
        "\n",
        "if not os.path.exists(resampled_folder):\n",
        "    os.makedirs(resampled_folder)\n",
        "\n",
        "# Resample all processed joint angles\n",
        "for filename in os.listdir(processed_folder):\n",
        "    if filename.endswith('_angles.npy'):\n",
        "        file_path = os.path.join(processed_folder, filename)\n",
        "        joint_angles = np.load(file_path)\n",
        "\n",
        "        # Resample joint angles to the target number of frames\n",
        "        resampled_angles = resample_joint_angles(joint_angles, target_frames)\n",
        "\n",
        "        # Save the resampled joint angles\n",
        "        output_path = os.path.join(resampled_folder, filename)\n",
        "        np.save(output_path, resampled_angles)\n",
        "        print(f\"Resampled and saved: {filename} (Original frames: {len(joint_angles)}, New frames: {len(resampled_angles)})\")\n",
        "\n",
        "print(\"All joint angles resampled.\")\n"
      ],
      "metadata": {
        "id": "RCRKwOIeqOee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audio"
      ],
      "metadata": {
        "id": "pLlmBQTtJyGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "import os\n",
        "\n",
        "# Path to directories\n",
        "audio_folder = '/content/drive/MyDrive/Output_features'\n",
        "output_folder = '/content/drive/MyDrive/Interpolated_Audio'\n",
        "desired_frames = 900\n",
        "\n",
        "def adjust_audio_features(audio_folder, output_folder, desired_frames):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(audio_folder):\n",
        "        if filename.endswith('.npz'):\n",
        "            file_path = os.path.join(audio_folder, filename)\n",
        "            output_file_path = os.path.join(output_folder, filename)\n",
        "\n",
        "            with np.load(file_path) as npz_data:\n",
        "                adjusted_data = {}\n",
        "\n",
        "                for key in npz_data:\n",
        "                    data = npz_data[key]\n",
        "                    original_shape = data.shape\n",
        "\n",
        "                    if key == 'tempo':\n",
        "                        # No interpolation needed for tempo\n",
        "                        adjusted_data[key] = data\n",
        "                    elif key == 'beat_frames':\n",
        "                        # Pad or crop beat_frames\n",
        "                        num_beats = original_shape[0]\n",
        "                        if num_beats > desired_frames:\n",
        "                            # Crop to desired frames\n",
        "                            adjusted_data[key] = data[:desired_frames]\n",
        "                        else:\n",
        "                            # Pad to desired frames\n",
        "                            adjusted_data[key] = np.pad(data, (0, desired_frames - num_beats), 'constant')\n",
        "                    elif len(original_shape) == 2:\n",
        "                        num_frames = original_shape[1]\n",
        "                        if num_frames == desired_frames:\n",
        "                            # No adjustment needed\n",
        "                            adjusted_data[key] = data\n",
        "                        else:\n",
        "                            # Interpolate to the desired number of frames\n",
        "                            x = np.linspace(0, 1, num_frames)\n",
        "                            new_x = np.linspace(0, 1, desired_frames)\n",
        "                            interpolator = interp1d(x, data, kind='cubic', axis=1, fill_value='extrapolate')\n",
        "                            adjusted_data[key] = interpolator(new_x)\n",
        "                    else:\n",
        "                        # Handle unexpected shapes\n",
        "                        print(f\"Unexpected shape for {key}: {original_shape}\")\n",
        "                        adjusted_data[key] = data\n",
        "\n",
        "                # Save the adjusted data\n",
        "                np.savez(output_file_path, **adjusted_data)\n",
        "                print(f'Saved adjusted file: {output_file_path}')\n",
        "\n",
        "# Adjust and save audio feature files\n",
        "adjust_audio_features(audio_folder, output_folder, desired_frames)\n"
      ],
      "metadata": {
        "id": "cJGjSdh1qee9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalization"
      ],
      "metadata": {
        "id": "idOOQ7DE7bIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sine-Cosine Normalization : video"
      ],
      "metadata": {
        "id": "u604ewKBJeb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def normalize_video_data(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.endswith('_angles.npy'):\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            try:\n",
        "                angles = np.load(file_path)\n",
        "                if angles.size == 0:\n",
        "                    print(f\"Skipping empty file: {file}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert angles from degrees to radians\n",
        "                angles_rad = np.deg2rad(angles)\n",
        "\n",
        "                # Apply sine-cosine normalization\n",
        "                normalized_angles = np.stack([\n",
        "                    np.sin(angles_rad),  # Sine of angles\n",
        "                    np.cos(angles_rad)   # Cosine of angles\n",
        "                ], axis=-1)\n",
        "\n",
        "                output_path = os.path.join(output_folder, file)\n",
        "                np.save(output_path, normalized_angles)\n",
        "                print(f\"Normalized and saved: {file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "input_video_folder = '/content/drive/MyDrive/Interpolated_Angles'\n",
        "normalized_video_folder = '/content/drive/MyDrive/Normalized_Video_Angles'\n",
        "normalize_video_data(input_video_folder, normalized_video_folder)\n",
        "print(\"All video data normalized.\")\n"
      ],
      "metadata": {
        "id": "eWbiancVyMoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max normalization : Video\n"
      ],
      "metadata": {
        "id": "QgPv6yh58Tfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def calculate_global_audio_min_max(input_folder, target_columns):\n",
        "    global_min_max = {}\n",
        "    print(\"Calculating global min and max values...\")\n",
        "\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.endswith('_features.npz'):\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            try:\n",
        "                data = np.load(file_path)\n",
        "                for key in data.keys():\n",
        "                    feature = data[key]\n",
        "\n",
        "                    # Ensure feature has 2 dimensions\n",
        "                    if feature.ndim == 1:\n",
        "                        feature = feature[np.newaxis, :]  # Add an extra dimension for consistency\n",
        "\n",
        "                    # Normalize the feature dimension\n",
        "                    feature = trim_or_pad_feature(feature, target_columns)\n",
        "\n",
        "                    # Calculate min and max values along the feature dimension\n",
        "                    min_vals = feature.min(axis=0, keepdims=True)\n",
        "                    max_vals = feature.max(axis=0, keepdims=True)\n",
        "\n",
        "                    if key not in global_min_max:\n",
        "                        global_min_max[key] = [min_vals, max_vals]\n",
        "                    else:\n",
        "                        global_min_max[key][0] = np.minimum(global_min_max[key][0], min_vals)\n",
        "                        global_min_max[key][1] = np.maximum(global_min_max[key][1], max_vals)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    print(\"Global min and max values calculated.\")\n",
        "    return global_min_max\n",
        "\n",
        "def normalize_audio_data(input_folder, output_folder, global_min_max, target_columns):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    print(\"Normalizing audio data...\")\n",
        "\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.endswith('_features.npz'):\n",
        "            file_path = os.path.join(input_folder, file)\n",
        "            try:\n",
        "                data = np.load(file_path)\n",
        "\n",
        "                normalized_data = {}\n",
        "                for key in data.keys():\n",
        "                    feature = data[key]\n",
        "\n",
        "                    # Ensure feature has 2 dimensions\n",
        "                    if feature.ndim == 1:\n",
        "                        feature = feature[np.newaxis, :]  # Add an extra dimension for consistency\n",
        "\n",
        "                    # Trim or pad feature to have the target number of columns\n",
        "                    feature = trim_or_pad_feature(feature, target_columns)\n",
        "\n",
        "                    min_vals = global_min_max[key][0]\n",
        "                    max_vals = global_min_max[key][1]\n",
        "\n",
        "                    # Normalize the feature\n",
        "                    normalized_feature = (feature - min_vals) / (max_vals - min_vals)\n",
        "                    normalized_feature = np.clip(normalized_feature, 0, 1)\n",
        "\n",
        "                    normalized_data[key] = normalized_feature\n",
        "\n",
        "                output_path = os.path.join(output_folder, file)\n",
        "                np.savez(output_path, **normalized_data)\n",
        "                print(f\"Normalized and saved: {file}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    print(\"All audio data normalized.\")\n",
        "\n",
        "def trim_or_pad_feature(feature, target_columns):\n",
        "    \"\"\"\n",
        "    Trim or pad the feature array to ensure it has exactly target_columns columns.\n",
        "    \"\"\"\n",
        "    current_columns = feature.shape[1]\n",
        "\n",
        "    if current_columns > target_columns:\n",
        "        # Trim columns\n",
        "        feature = feature[:, :target_columns]\n",
        "    elif current_columns < target_columns:\n",
        "        # Pad columns with zeros\n",
        "        padding = target_columns - current_columns\n",
        "        feature = np.pad(feature, ((0, 0), (0, padding)), mode='constant')\n",
        "\n",
        "    return feature\n",
        "\n",
        "input_audio_folder = '/content/drive/MyDrive/Interpolated_Audio'\n",
        "normalized_audio_folder = '/content/drive/MyDrive/Normalized_Features'\n",
        "target_columns = 900\n",
        "\n",
        "global_audio_min_max = calculate_global_audio_min_max(input_audio_folder, target_columns)\n",
        "normalize_audio_data(input_audio_folder, normalized_audio_folder, global_audio_min_max, target_columns)\n"
      ],
      "metadata": {
        "id": "IH9vYdRByytH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synchronization of Data\n"
      ],
      "metadata": {
        "id": "It2--1HB-kFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def synchronize_data(video_folder, audio_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    video_files = [file for file in os.listdir(video_folder) if file.endswith('_angles.npy')]\n",
        "    total_files = len(video_files)\n",
        "    synchronized_files = 0\n",
        "    unsynchronized_files = []\n",
        "\n",
        "    for i, file in enumerate(video_files):\n",
        "        video_path = os.path.join(video_folder, file)\n",
        "        audio_path = os.path.join(audio_folder, file.replace('_angles.npy', '_features.npz'))\n",
        "\n",
        "        if os.path.exists(audio_path):\n",
        "            try:\n",
        "                video_data = np.load(video_path)\n",
        "                audio_data = np.load(audio_path)\n",
        "\n",
        "                # Ensure video and audio data have the expected shapes\n",
        "                if len(video_data.shape) != 2 or video_data.shape[1] != 12:\n",
        "                    raise ValueError(f\"Unexpected shape for video data in {file}: {video_data.shape}\")\n",
        "\n",
        "                for key in audio_data.keys():\n",
        "                    if len(audio_data[key].shape) != 2:\n",
        "                        raise ValueError(f\"Unexpected shape for audio feature {key} in {file}: {audio_data[key].shape}\")\n",
        "\n",
        "                synchronized_data = {\n",
        "                    'video': video_data,\n",
        "                    'audio': {key: audio_data[key] for key in audio_data.keys()}\n",
        "                }\n",
        "\n",
        "                output_path = os.path.join(output_folder, file.replace('_angles.npy', '_synchronized.npz'))\n",
        "                np.savez(output_path, **synchronized_data)\n",
        "                synchronized_files += 1\n",
        "                print(f\"Synchronized and saved: {file} ({i + 1}/{total_files})\")\n",
        "            except Exception as e:\n",
        "                unsynchronized_files.append(file)\n",
        "                print(f\"Error processing {file}: {e}\")\n",
        "        else:\n",
        "            unsynchronized_files.append(file)\n",
        "            print(f\"Could not find matching audio for: {file} ({i + 1}/{total_files})\")\n",
        "\n",
        "    print(f\"\\nTotal files found: {total_files}\")\n",
        "    print(f\"Total files synchronized: {synchronized_files}\")\n",
        "    print(f\"Files that could not be synchronized: {len(unsynchronized_files)}\")\n",
        "    if unsynchronized_files:\n",
        "        print(\"\\nList of files that could not be synchronized:\")\n",
        "        for file in unsynchronized_files:\n",
        "            print(file)\n",
        "\n",
        "# Folders\n",
        "video_folder = '/content/drive/MyDrive/NEW_Normalized_Video_Angles'\n",
        "audio_folder = '/content/drive/MyDrive/Normalized_Features'\n",
        "output_folder = '/content/drive/MyDrive/NEW_Synchronized_Data'\n",
        "\n",
        "# Synchronize data\n",
        "synchronize_data(video_folder, audio_folder, output_folder)\n",
        "print(\"All data synchronized.\")"
      ],
      "metadata": {
        "id": "PioYncnn4uP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some important libraries\n"
      ],
      "metadata": {
        "id": "-FsE02cgA2Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "m4vkL1M1A-L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing\n"
      ],
      "metadata": {
        "id": "lQAtFMgrAbWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class SynchronizedAudioDanceDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.file_list = [f for f in os.listdir(root_dir) if f.endswith('.npz')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = os.path.join(self.root_dir, self.file_list[idx])\n",
        "        Sync_data = np.load(file_path, allow_pickle=True)\n",
        "        audio_data = Sync_data['audio'].item()\n",
        "        video_data = Sync_data['video']\n",
        "\n",
        "        audio_features = {}\n",
        "        for key in ['tempo','mel_spectrogram', 'mfcc', 'beat_frames', 'chroma']:\n",
        "            try:\n",
        "                tensor = torch.tensor(audio_data[key]).float()\n",
        "                tensor = torch.nan_to_num(tensor)\n",
        "                tensor = torch.clamp(tensor, -1e6, 1e6)\n",
        "                if tensor.shape[-1] > 900:\n",
        "                    tensor = nn.functional.interpolate(tensor.unsqueeze(0), size=900, mode='linear', align_corners=False).squeeze(0)\n",
        "                audio_features[key] = tensor\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {key}: {e}\")\n",
        "                print(f\"Shape of {key}: {np.array(audio_data[key]).shape}\")\n",
        "\n",
        "        video_features = torch.tensor(video_data).float()\n",
        "        if video_features.shape[0] > 900:\n",
        "            video_features = nn.functional.interpolate(video_features.unsqueeze(0).unsqueeze(0),\n",
        "                                                      size=(900, 12, 2),\n",
        "                                                      mode='trilinear',\n",
        "                                                      align_corners=False).squeeze(0).squeeze(0)\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "class AugmentedSynchronizedAudioDanceDataset(SynchronizedAudioDanceDataset):\n",
        "    def __init__(self, root_dir, augment_prob=0.7):\n",
        "        super().__init__(root_dir)\n",
        "        self.augment_prob = augment_prob\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_features, video_features = super().__getitem__(idx)\n",
        "\n",
        "        if random.random() < self.augment_prob:\n",
        "            audio_features, video_features = self.augment(audio_features, video_features)\n",
        "\n",
        "        return audio_features, video_features\n",
        "    def time_warp(self, audio_features, video_features, max_time_warp=50): # Added self argument\n",
        "        t = video_features.shape[1]\n",
        "        if t <= max_time_warp:\n",
        "            return audio_features, video_features\n",
        "\n",
        "        t1 = random.randint(max_time_warp, t - max_time_warp)\n",
        "        scale = random.uniform(0.8, 1.25)\n",
        "        t1_new = int(t1 * scale)\n",
        "\n",
        "        # Warp the first part\n",
        "        video_warped_1 = nn.functional.interpolate(video_features[:, :t1].permute(0, 3, 1, 2), size=(t1_new, 12), mode='bilinear', align_corners=False).permute(0, 2, 3, 1)\n",
        "\n",
        "        # Keep the second part unchanged\n",
        "        video_warped_2 = video_features[:, t1:]\n",
        "\n",
        "        # Interpolate or truncate to maintain original length\n",
        "        if t1_new + video_warped_2.shape[1] > t:\n",
        "            video_features = torch.cat([video_warped_1, video_warped_2], dim=1)[:, :t]\n",
        "        else:\n",
        "            padding = t - (t1_new + video_warped_2.shape[1])\n",
        "            video_features = nn.functional.pad(torch.cat([video_warped_1, video_warped_2], dim=1), (0, 0, 0, 0, 0, padding))\n",
        "\n",
        "        for key in audio_features:\n",
        "            if key not in ['tempo', 'beat_frames']:\n",
        "                # Warp the first part\n",
        "                audio_warped_1 = nn.functional.interpolate(audio_features[key][:, :t1].unsqueeze(1), size=t1_new, mode='cubic', align_corners=False).squeeze(1)\n",
        "\n",
        "                # Keep the second part unchanged\n",
        "                audio_warped_2 = audio_features[key][:, t1:]\n",
        "\n",
        "                # Interpolate or truncate to maintain original length\n",
        "                if t1_new + audio_warped_2.shape[1] > t:\n",
        "                    audio_features[key] = torch.cat([audio_warped_1, audio_warped_2], dim=1)[:, :t]\n",
        "                else:\n",
        "                    padding = t - (t1_new + audio_warped_2.shape[1])\n",
        "                    audio_features[key] = nn.functional.pad(torch.cat([audio_warped_1, audio_warped_2], dim=1), (0, padding))\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "    # Add this to your augment method in AugmentedSynchronizedAudioDanceDataset\n",
        "    def augment(self, audio_features, video_features):\n",
        "        augmentations = [\n",
        "            self.frequency_masking,\n",
        "            self.add_noise,\n",
        "            self.random_invert,\n",
        "            self.random_scale,\n",
        "            self.time_warp  # Add this new augmentation\n",
        "        ]\n",
        "\n",
        "        num_augmentations = random.randint(1, 2)\n",
        "        chosen_augmentations = random.sample(augmentations, num_augmentations)\n",
        "\n",
        "        for aug_func in chosen_augmentations:\n",
        "            audio_features, video_features = aug_func(audio_features, video_features)\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "    def frequency_masking(self, audio_features, video_features, max_mask_size=20):\n",
        "        for key in ['mfcc', 'chroma', 'spectral_contrast', 'tonnetz', 'mel_spectrogram']:\n",
        "            if key in audio_features:\n",
        "                feature_size = audio_features[key].shape[0]\n",
        "                if feature_size > 1:\n",
        "                    mask_size = min(random.randint(1, max_mask_size), feature_size - 1)\n",
        "                    start = random.randint(0, feature_size - mask_size)\n",
        "                    audio_features[key][start:start+mask_size, :] = 0\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "    def add_noise(self, audio_features, video_features, noise_level=0.005):\n",
        "        for key in audio_features:\n",
        "            noise = torch.randn_like(audio_features[key]) * noise_level\n",
        "            audio_features[key] += noise\n",
        "\n",
        "        video_noise = torch.randn_like(video_features) * noise_level\n",
        "        video_features += video_noise\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "    def random_invert(self, audio_features, video_features):\n",
        "        if random.random() < 0.5:\n",
        "            for key in audio_features:\n",
        "                if key not in ['tempo', 'beat_frames']:\n",
        "                    audio_features[key] = -audio_features[key]\n",
        "\n",
        "            video_features = -video_features\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "    def random_scale(self, audio_features, video_features, max_scale=0.2):\n",
        "        scale_factor = 1 + random.uniform(-max_scale, max_scale)\n",
        "\n",
        "        for key in audio_features:\n",
        "            if key not in ['tempo', 'beat_frames']:\n",
        "                audio_features[key] *= scale_factor\n",
        "\n",
        "        video_features *= scale_factor\n",
        "\n",
        "        return audio_features, video_features\n",
        "\n",
        "# Use the augmented dataset\n",
        "\n",
        "dataset = AugmentedSynchronizedAudioDanceDataset('/content/drive/MyDrive/Synchronised_Data')\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "uhrTaEMlAka9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "slrrzsnn_boM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Architecture"
      ],
      "metadata": {
        "id": "7FLPGRnLBIff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=900):\n",
        "        super().__init__()\n",
        "        self.encoding = torch.zeros(1, max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        self.encoding[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[0, :, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "class DanceGenerator(nn.Module):\n",
        "    def __init__(self, audio_dim, video_dim, hidden_dim, num_heads, num_layers, target_seq_len, lstm_layers=2, lstm_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.audio_encoder = nn.Linear(audio_dim, hidden_dim)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout)\n",
        "\n",
        "        # Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Transformer Decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim, video_dim * 2)\n",
        "        self.target_seq_len = target_seq_len\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, audio_features):\n",
        "        audio_features = audio_features.permute(0, 2, 1)\n",
        "        encoded_audio = self.audio_encoder(audio_features)\n",
        "        encoded_audio = self.dropout(encoded_audio)\n",
        "        encoded_audio = self.positional_encoding(encoded_audio)\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_output, _ = self.lstm(encoded_audio)\n",
        "\n",
        "        # Encoder\n",
        "        memory = self.transformer_encoder(lstm_output)\n",
        "\n",
        "        # Initialize target sequence for decoder\n",
        "        batch_size = audio_features.size(0)\n",
        "        target = torch.zeros(batch_size, self.target_seq_len, encoded_audio.size(-1)).to(audio_features.device)\n",
        "        target = self.positional_encoding(target)\n",
        "\n",
        "        # Decoder\n",
        "        output = self.transformer_decoder(target, memory)\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        # Final output layer\n",
        "        output = self.fc(output)\n",
        "        output = output.view(output.size(0), output.size(1), -1, 2)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "M-0mjIRy_6K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "yFiwYFivHHf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_dim = 128 + 13 + 12 + 1 + 1\n",
        "video_dim = 12\n",
        "hidden_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 6\n",
        "batch_size = 4\n",
        "num_epochs = 50\n",
        "learning_rate = 0.01\n",
        "accuracy_threshold = 0.18\n",
        "target_seq_len = 900\n",
        "\n",
        "# New hyperparameters for LSTM\n",
        "lstm_layers = 3\n",
        "lstm_dropout = 0.1\n",
        "\n",
        "model = DanceGenerator(audio_dim, video_dim, hidden_dim, num_heads, num_layers, target_seq_len,\n",
        "                       lstm_layers=lstm_layers, lstm_dropout=lstm_dropout).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)"
      ],
      "metadata": {
        "id": "Zh2U80qaGL4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and evaluation Functions"
      ],
      "metadata": {
        "id": "cHb5Oz5zHLZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_invalid_values(tensor, name):\n",
        "    if torch.isnan(tensor).any():\n",
        "        print(f\"NaN values found in {name}\")\n",
        "    if torch.isinf(tensor).any():\n",
        "        print(f\"Infinite values found in {name}\")\n",
        "\n",
        "def calculate_accuracy(predictions, targets, threshold):\n",
        "    abs_errors = np.abs(predictions - targets)\n",
        "    accuracy = np.mean(abs_errors <= threshold)\n",
        "    return accuracy\n",
        "\n",
        "def calculate_rhythm_matching(predictions, targets):\n",
        "    predictions = np.array(predictions)\n",
        "    targets = np.array(targets)\n",
        "    return np.mean(np.abs(np.diff(predictions, axis=1) - np.diff(targets, axis=1)))\n",
        "\n",
        "def calculate_style_consistency(predictions, targets):\n",
        "    predictions = np.array(predictions)\n",
        "    targets = np.array(targets)\n",
        "    return np.mean(np.std(predictions, axis=1) / np.std(targets, axis=1))\n",
        "\n",
        "def diversity_loss(output):\n",
        "    diff = output[:, 1:] - output[:, :-1]\n",
        "    return -torch.mean(torch.abs(diff))\n",
        "\n",
        "def temporal_loss(output, target, frame_window=30):\n",
        "    temporal_diff = torch.abs(output[:, 1:] - output[:, :-1] - (target[:, 1:] - target[:, :-1]))\n",
        "    direction_change_penalty = torch.zeros_like(temporal_diff)\n",
        "\n",
        "    for i in range(2, frame_window + 1):\n",
        "        direction_change_penalty[:, i-1:] += torch.abs(output[:, i:] - 2 * output[:, i-1:-1] + output[:, i-2:-2])\n",
        "\n",
        "    return torch.mean(temporal_diff) + torch.mean(direction_change_penalty)\n",
        "\n",
        "# Now use this updated temporal loss in your combined loss function\n",
        "def combined_loss(output, target):\n",
        "    mse_loss = criterion(output, target)\n",
        "    temp_loss = temporal_loss(output, target)\n",
        "    div_loss = diversity_loss(output)\n",
        "    return mse_loss + 0.1 * temp_loss + 0.05 * div_loss"
      ],
      "metadata": {
        "id": "7b1S-IAGGMhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining terms"
      ],
      "metadata": {
        "id": "KqxukwkRHV68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_mae = []\n",
        "test_mae = []\n",
        "train_r2 = []\n",
        "test_r2 = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "train_rhythm_matching = []\n",
        "test_rhythm_matching = []\n",
        "train_style_consistency = []\n",
        "test_style_consistency = []\n"
      ],
      "metadata": {
        "id": "gOU74JsUGS9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop"
      ],
      "metadata": {
        "id": "gtyxmEvAHX0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    train_outputs = []\n",
        "    train_targets = []\n",
        "\n",
        "    for audio_features, video_features in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\", unit=\"batch\"):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        combined_audio = torch.cat([\n",
        "            audio_features['tempo'].view(-1, 1, 900),\n",
        "            audio_features['mel_spectrogram'].view(-1, 128, 900),\n",
        "            audio_features['mfcc'].view(-1, 13, 900),\n",
        "            audio_features['chroma'].view(-1, 12, 900),\n",
        "            audio_features['beat_frames'].view(-1, 1, 900),\n",
        "        ], dim=1).to(device)\n",
        "\n",
        "        output = model(combined_audio)\n",
        "        check_invalid_values(output, 'output')\n",
        "\n",
        "        loss = combined_loss(output, video_features.to(device))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        train_outputs.append(output.cpu().detach().numpy())\n",
        "        train_targets.append(video_features.numpy())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    train_outputs = np.concatenate(train_outputs, axis=0).reshape(-1, 900, 12, 2)\n",
        "    train_targets = np.concatenate(train_targets, axis=0).reshape(-1, 900, 12, 2)\n",
        "\n",
        "    train_outputs_flat = train_outputs.reshape(-1, 2)\n",
        "    train_targets_flat = train_targets.reshape(-1, 2)\n",
        "\n",
        "    train_mae.append(mean_absolute_error(train_targets_flat, train_outputs_flat))\n",
        "    train_r2.append(r2_score(train_targets_flat, train_outputs_flat))\n",
        "    train_accuracy.append(calculate_accuracy(train_outputs, train_targets, accuracy_threshold))\n",
        "    train_rhythm_matching.append(calculate_rhythm_matching(train_outputs, train_targets))\n",
        "    train_style_consistency.append(calculate_style_consistency(train_outputs, train_targets))\n",
        "\n",
        "    model.eval()\n",
        "    total_test_loss = 0\n",
        "    test_outputs = []\n",
        "    test_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for audio_features, video_features in tqdm(test_dataloader, desc=f\"Testing Epoch {epoch+1}\", unit=\"batch\"):\n",
        "            combined_audio = torch.cat([\n",
        "                audio_features['tempo'].view(-1,1,900),\n",
        "                audio_features['mel_spectrogram'].view(-1, 128, 900),\n",
        "                audio_features['mfcc'].view(-1, 13, 900),\n",
        "                audio_features['chroma'].view(-1, 12, 900),\n",
        "                audio_features['beat_frames'].view(-1, 1, 900),\n",
        "            ], dim=1).to(device)\n",
        "\n",
        "            output = model(combined_audio)\n",
        "            check_invalid_values(output, 'output')\n",
        "\n",
        "            loss = combined_loss(output, video_features.to(device))\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            test_outputs.append(output.cpu().detach().numpy())\n",
        "            test_targets.append(video_features.numpy())\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    test_losses.append(avg_test_loss)\n",
        "\n",
        "    test_outputs = np.concatenate(test_outputs, axis=0).reshape(-1, 900, 12, 2)\n",
        "    test_targets = np.concatenate(test_targets, axis=0).reshape(-1, 900, 12, 2)\n",
        "\n",
        "    test_outputs_flat = test_outputs.reshape(-1, 2)\n",
        "    test_targets_flat = test_targets.reshape(-1, 2)\n",
        "\n",
        "    test_mae.append(mean_absolute_error(test_targets_flat, test_outputs_flat))\n",
        "    test_r2.append(r2_score(test_targets_flat, test_outputs_flat))\n",
        "    test_accuracy.append(calculate_accuracy(test_outputs, test_targets, accuracy_threshold))\n",
        "    test_rhythm_matching.append(calculate_rhythm_matching(test_outputs, test_targets))\n",
        "    test_style_consistency.append(calculate_style_consistency(test_outputs, test_targets))\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
        "    print(f\"Train MAE: {train_mae[-1]:.4f}, Test MAE: {test_mae[-1]:.4f}\")\n",
        "    print(f\"Train R2: {train_r2[-1]:.4f}, Test R2: {test_r2[-1]:.4f}\")\n",
        "    print(f\"Train Accuracy: {train_accuracy[-1]:.4f}, Test Accuracy: {test_accuracy[-1]:.4f}\")\n",
        "    print(f\"Train Rhythm Matching: {train_rhythm_matching[-1]:.4f}, Test Rhythm Matching: {test_rhythm_matching[-1]:.4f}\")\n",
        "    print(f\"Train Style Consistency: {train_style_consistency[-1]:.4f}, Test Style Consistency: {test_style_consistency[-1]:.4f}\")\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "zbotKH-RGfiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting of graphs"
      ],
      "metadata": {
        "id": "oDXtZ8JdHZej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(test_losses, label='Test Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the MAE\n",
        "plt.figure()\n",
        "plt.plot(train_mae, label='Train MAE')\n",
        "plt.plot(test_mae, label='Test MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the R2 Score\n",
        "plt.figure()\n",
        "plt.plot(train_r2, label='Train R2')\n",
        "plt.plot(test_r2, label='Test R2')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('R2 Score')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the Accuracy\n",
        "plt.figure()\n",
        "plt.plot(train_accuracy, label='Train Accuracy')\n",
        "plt.plot(test_accuracy, label='Test Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the Rhythm Matching\n",
        "plt.figure()\n",
        "plt.plot(train_rhythm_matching, label='Train Rhythm Matching')\n",
        "plt.plot(test_rhythm_matching, label='Test Rhythm Matching')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Rhythm Matching')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the Style Consistency\n",
        "plt.figure()\n",
        "plt.plot(train_style_consistency, label='Train Style Consistency')\n",
        "plt.plot(test_style_consistency, label='Test Style Consistency')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Style Consistency')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bgbbCO5cGjvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the Model"
      ],
      "metadata": {
        "id": "dMfwagK3HcRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/OutputModelsAndNPYs/LSTM.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/OutputModelsAndNPYs/LSTM_state_dict.pth')"
      ],
      "metadata": {
        "id": "iosNq20AGozj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation Of The Model"
      ],
      "metadata": {
        "id": "uzw7T0M8Hoeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for exection\n",
        "\n"
      ],
      "metadata": {
        "id": "sRhMqvmfHeZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(audio_path):\n",
        "    # Load your audio features here\n",
        "    # For this example, we'll assume the features are pre-extracted and stored in a dictionary format\n",
        "    # Replace this with your actual audio feature extraction code\n",
        "    audio_data = np.load(audio_path, allow_pickle=True)\n",
        "    audio_features = {\n",
        "        'beat_frames': torch.tensor(audio_data['beat_frames']).float(),\n",
        "        'mfcc': torch.tensor(audio_data['mfcc']).float(),\n",
        "        'chroma': torch.tensor(audio_data['chroma']).float(),\n",
        "        'spectral_contrast': torch.tensor(audio_data['spectral_contrast']).float(),\n",
        "        'mel_spectrogram': torch.tensor(audio_data['mel_spectrogram']).float(),\n",
        "    }\n",
        "    for key in audio_features:\n",
        "        audio_features[key] = torch.nan_to_num(audio_features[key])\n",
        "        audio_features[key] = torch.clamp(audio_features[key], -1e6, 1e6)\n",
        "    return audio_features\n",
        "\n",
        "# Function to generate dance moves\n",
        "def generate_dance_moves(model, audio_features, target_seq_len):\n",
        "    with torch.no_grad():\n",
        "        combined_audio = torch.cat([\n",
        "            audio_features['mel_spectrogram'].view(-1, 128, 900),\n",
        "            audio_features['mfcc'].view(-1, 13, 900),\n",
        "            audio_features['chroma'].view(-1, 12, 900),\n",
        "            audio_features['beat_frames'].view(-1, 1, 900),\n",
        "        ], dim=1).to(device)\n",
        "\n",
        "        output = model(combined_audio)\n",
        "        output = output.cpu().numpy()\n",
        "        return output"
      ],
      "metadata": {
        "id": "woGxdB47GsNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model execution"
      ],
      "metadata": {
        "id": "YJnqu2xMHkN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = DanceGenerator(audio_dim, video_dim, hidden_dim, num_heads, num_layers, target_seq_len,\n",
        "                       lstm_layers=lstm_layers, lstm_dropout=lstm_dropout).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Preprocess the input audio file\n",
        "input_audio_path = '/content/drive/MyDrive/Normalized_Features/P5_features.npz'  # Replace with your input audio features file path\n",
        "audio_features = preprocess_audio(input_audio_path)\n",
        "\n",
        "# Generate dance moves\n",
        "generated_dance_moves = generate_dance_moves(model, audio_features, 900)\n",
        "\n",
        "# Save the generated dance moves to a .npy file\n",
        "output_file_path = '/content/drive/MyDrive/OutputModelsAndNPYs/LSTM_Dance.npy'\n",
        "np.save(output_file_path, generated_dance_moves)\n",
        "print(f\"Generated dance moves saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "zJ2t2o2DGvIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for simulation"
      ],
      "metadata": {
        "id": "1RtaUjQrHtPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_joint_position(start_pos, angle, length, invert_y=False):\n",
        "    \"\"\" Calculate the joint position given an angle and length. \"\"\"\n",
        "    angle_rad = np.radians(angle)\n",
        "    x = start_pos[0] + length * np.cos(angle_rad)\n",
        "    y = start_pos[1] - length * np.sin(angle_rad) if not invert_y else start_pos[1] + length * np.sin(angle_rad)\n",
        "    return (int(x), int(y))\n",
        "\n",
        "def regenerate_pose_sin_cos(angle_data_file, output_video_path, frame_rate=30):\n",
        "    angles_data = np.load(angle_data_file, allow_pickle=True)\n",
        "    print(f\"Angles data loaded: {angles_data.shape}\")\n",
        "    print(angles_data)\n",
        "    angles_data=angles_data[0]\n",
        "    # Convert sine-cosine pairs back to angles in degrees\n",
        "    angles_deg = np.degrees(np.arctan2(angles_data[..., 1], angles_data[..., 0]))\n",
        "    print(angles_deg)\n",
        "    # Define lengths of the limbs\n",
        "    upper_arm_length, lower_arm_length, hand_length = 60, 50, 20\n",
        "    upper_leg_length, lower_leg_length, foot_length = 70, 60, 30\n",
        "    torso_length, neck_length = 100, 20\n",
        "    head_radius = 20\n",
        "    hip_width, shoulder_width = 60, 80\n",
        "\n",
        "    # Create a blank image for visualization\n",
        "    height, width = 720, 1280\n",
        "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, (width, height))\n",
        "\n",
        "    total_frames = angles_deg.shape[0]\n",
        "    print(f\"Total frames: {total_frames}\")\n",
        "\n",
        "    for frame_idx in range(total_frames):\n",
        "        if frame_idx % 100 == 0:\n",
        "            print(f\"Processing frame {frame_idx}/{total_frames}\")\n",
        "\n",
        "        frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        center_x, center_y = width // 2, height // 2\n",
        "\n",
        "        angles = angles_deg[frame_idx]\n",
        "\n",
        "        # Unpack all angles\n",
        "        left_shoulder_angle, left_elbow_angle, left_wrist_angle, \\\n",
        "        right_shoulder_angle, right_elbow_angle, right_wrist_angle, \\\n",
        "        left_hip_angle, left_knee_angle, left_ankle_angle, \\\n",
        "        right_hip_angle, right_knee_angle, right_ankle_angle = angles\n",
        "\n",
        "        # Define the base positions\n",
        "        base_pos = (center_x, center_y + 200)  # Move the base position lower\n",
        "\n",
        "        # Calculate hip positions\n",
        "        left_hip_pos = calculate_joint_position(base_pos, left_hip_angle, hip_width // 2)\n",
        "        right_hip_pos = calculate_joint_position(base_pos, right_hip_angle, hip_width // 2)\n",
        "\n",
        "        # Calculate torso position (midpoint between hips)\n",
        "        torso_pos = ((left_hip_pos[0] + right_hip_pos[0]) // 2,\n",
        "                     (left_hip_pos[1] + right_hip_pos[1]) // 2 - torso_length)\n",
        "\n",
        "        # Calculate shoulder positions\n",
        "        left_shoulder_pos = (torso_pos[0] - shoulder_width // 2, torso_pos[1])\n",
        "        right_shoulder_pos = (torso_pos[0] + shoulder_width // 2, torso_pos[1])\n",
        "\n",
        "        # Calculate the head and neck positions\n",
        "        neck_pos = (torso_pos[0], torso_pos[1] - neck_length)\n",
        "        head_pos = (neck_pos[0], neck_pos[1] - head_radius)\n",
        "\n",
        "        # Calculate arm positions\n",
        "        left_elbow_pos = calculate_joint_position(left_shoulder_pos, -left_shoulder_angle, upper_arm_length)\n",
        "        left_wrist_pos = calculate_joint_position(left_elbow_pos, left_elbow_angle, lower_arm_length)\n",
        "        left_hand_pos = calculate_joint_position(left_wrist_pos, left_wrist_angle, hand_length)\n",
        "\n",
        "        right_elbow_pos = calculate_joint_position(right_shoulder_pos, -right_shoulder_angle, upper_arm_length)\n",
        "        right_wrist_pos = calculate_joint_position(right_elbow_pos, -right_elbow_angle, lower_arm_length)\n",
        "        right_hand_pos = calculate_joint_position(right_wrist_pos, -right_wrist_angle, hand_length)\n",
        "\n",
        "        # Calculate leg positions\n",
        "        left_knee_pos = calculate_joint_position(left_hip_pos, left_hip_angle, upper_leg_length)\n",
        "        left_ankle_pos = calculate_joint_position(left_knee_pos, left_knee_angle, lower_leg_length)\n",
        "        left_foot_pos = calculate_joint_position(left_ankle_pos, left_ankle_angle, foot_length)\n",
        "\n",
        "        right_knee_pos = calculate_joint_position(right_hip_pos, -right_hip_angle, upper_leg_length)\n",
        "        right_ankle_pos = calculate_joint_position(right_knee_pos, -right_knee_angle, lower_leg_length)\n",
        "        right_foot_pos = calculate_joint_position(right_ankle_pos, -right_ankle_angle, foot_length)\n",
        "\n",
        "        # Draw the joints and connections\n",
        "        joints = [\n",
        "            (left_shoulder_pos, left_elbow_pos),\n",
        "            (left_elbow_pos, left_wrist_pos),\n",
        "            (left_wrist_pos, left_hand_pos),\n",
        "            (right_shoulder_pos, right_elbow_pos),\n",
        "            (right_elbow_pos, right_wrist_pos),\n",
        "            (right_wrist_pos, right_hand_pos),\n",
        "            (left_hip_pos, left_knee_pos),\n",
        "            (left_knee_pos, left_ankle_pos),\n",
        "            (left_ankle_pos, left_foot_pos),\n",
        "            (right_hip_pos, right_knee_pos),\n",
        "            (right_knee_pos, right_ankle_pos),\n",
        "            (right_ankle_pos, right_foot_pos),\n",
        "            (left_shoulder_pos, right_shoulder_pos),\n",
        "            (left_hip_pos, right_hip_pos),\n",
        "            (left_shoulder_pos, left_hip_pos),\n",
        "            (right_shoulder_pos, right_hip_pos),\n",
        "            (torso_pos, neck_pos),\n",
        "            (neck_pos, head_pos),\n",
        "            #(base_pos, left_hip_pos),\n",
        "            #(base_pos, right_hip_pos)\n",
        "        ]\n",
        "\n",
        "        for start_pos, end_pos in joints:\n",
        "            cv2.line(frame, start_pos, end_pos, (0, 255, 0), 2)\n",
        "            cv2.circle(frame, start_pos, 5, (0, 0, 255), -1)\n",
        "            cv2.circle(frame, end_pos, 5, (0, 0, 255), -1)\n",
        "\n",
        "        # Draw the head as a circle\n",
        "        cv2.circle(frame, head_pos, head_radius, (255, 0, 0), 2)\n",
        "\n",
        "        # Display the frame number\n",
        "        cv2.putText(frame, f'Frame: {frame_idx+1}/{total_frames}',\n",
        "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "\n",
        "    out.release()\n",
        "    print(f\"Video generation complete. Output saved to {output_video_path}\")"
      ],
      "metadata": {
        "id": "9bn5Jl4JH0rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulation"
      ],
      "metadata": {
        "id": "_a2-g-guH75K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regenerate_pose_sin_cos(r\"C:\\Users\\User\\Downloads\\LSTM_dance.npy\", 'okay.mp4', frame_rate=30)"
      ],
      "metadata": {
        "id": "rmYzmBjHH-Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Thank you!***"
      ],
      "metadata": {
        "id": "MTkrRC3SICvk"
      }
    }
  ]
}